### ğŸ Web Scraping & Data Analysis Project (Python)

#### ğŸ“„ Project Overview
This project demonstrates how to perform **web scraping** using `BeautifulSoup` and organize the extracted data using **Pandas** in a **Jupyter Notebook** environment. The aim is to retrieve structured data from a website and conduct basic exploratory data analysis (EDA).

#### ğŸ› ï¸ Tools & Libraries Used
- **Python**
- **Jupyter Notebook**
- `requests` â€“ for sending HTTP requests
- `BeautifulSoup` â€“ for parsing HTML and scraping data
- `pandas` â€“ for data cleaning, transformation, and analysis

#### ğŸ“Š Project Workflow
1. **Selected a website** to scrape data ([List_of_largest_companies_in_the_United_States_by_revenue](https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue))
2. Sent requests and parsed the webpage using `BeautifulSoup`
3. Extracted relevant data such as titles, prices, tags, or table contents
4. Converted the extracted data into a **structured pandas DataFrame**
5. Saved the DataFrame as a CSV file (`Largest_Companies.csv`)
6. Performed **cleaning, formatting, and basic EDA** (null handling, deduplication, filtering, etc.)

#### ğŸ” What I Learned
- How to fetch and parse web data using `BeautifulSoup`
- Navigating and extracting information from HTML tags
- Structuring and organizing messy web data using `pandas`
- Working with **real-world unstructured data** in Python

#### ğŸ“ How to Use
1. Clone this repository
2. Open the Jupyter Notebook (`Scraping Data from a Real Website + Pandas.ipynb` file)
3. Run each cell to scrape, process, and analyze the data
4. Customize the scraper for other websites as needed


